"""
æ™ºèƒ½æœç´¢æœåŠ¡æ¨¡å—
ä½¿ç”¨Function Callingè®©å¤§æ¨¡å‹è‡ªä¸»å†³å®šæœç´¢ç­–ç•¥
"""

import asyncio
import json
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.models import SearchResults, SearchResult
from app.services.search_service import SearchService
from app.services.llm_service import LLMService
from app.utils import get_settings, LoggerMixin, SearchException


class IntelligentSearchService(LoggerMixin):
    """
    æ™ºèƒ½æœç´¢æœåŠ¡ç±»
    ä½¿ç”¨Function Callingè®©å¤§æ¨¡å‹è‡ªä¸»å†³å®šæœç´¢ç­–ç•¥
    """
    
    def __init__(self):
        self.settings = get_settings()
        self.search_service = SearchService()
        self.llm_service = LLMService()
        
        # å®šä¹‰æœç´¢å·¥å…·
        self.search_tools = [
            {
                "type": "function",
                "function": {
                    "name": "search_web",
                    "description": "æœç´¢äº’è”ç½‘è·å–ç›¸å…³èµ„æºå’Œä¿¡æ¯",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "æœç´¢æŸ¥è¯¢å…³é”®è¯"
                            },
                            "search_type": {
                                "type": "string",
                                "enum": ["arxiv_papers", "github_repos", "research_code", "academic_datasets", "conference_papers", "huggingface_models"],
                                "description": "æœç´¢ç±»å‹ï¼šarxiv_papers(arXivè®ºæ–‡)ã€github_repos(GitHubä»£ç åº“)ã€research_code(ç ”ç©¶ä»£ç )ã€academic_datasets(å­¦æœ¯æ•°æ®é›†)ã€conference_papers(ä¼šè®®è®ºæ–‡)ã€huggingface_models(Hugging Faceæ¨¡å‹)"
                            },
                            "max_results": {
                                "type": "integer",
                                "description": "æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤5",
                                "default": 5
                            }
                        },
                        "required": ["query", "search_type"]
                    }
                }
            }
        ]
    
    async def intelligent_search(
        self, 
        topic: str, 
        language: str = "zh",
        model: str = None
    ) -> SearchResults:
        """
        æ™ºèƒ½æœç´¢ï¼Œè®©å¤§æ¨¡å‹è‡ªä¸»å†³å®šæœç´¢ç­–ç•¥
        
        Args:
            topic: æœç´¢ä¸»é¢˜
            language: è¯­è¨€
            model: æŒ‡å®šçš„æ¨¡å‹
            
        Returns:
            SearchResults: èšåˆçš„æœç´¢ç»“æœ
        """
        start_time = datetime.now()
        self.logger.info(f"å¼€å§‹æ™ºèƒ½æœç´¢: {topic}")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šè®©å¤§æ¨¡å‹åˆ†æä¸»é¢˜å¹¶å†³å®šæœç´¢ç­–ç•¥
            search_plan = await self._generate_search_plan(topic, language, model)
            
            # ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œæœç´¢è®¡åˆ’
            all_results = []
            search_tasks = []
            
            for search_call in search_plan:
                task = self._execute_search_call(search_call)
                search_tasks.append(task)
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æœç´¢ä»»åŠ¡
            search_results_list = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            # åˆå¹¶æœç´¢ç»“æœ
            for results in search_results_list:
                if isinstance(results, SearchResults):
                    all_results.extend(results.results)
                elif isinstance(results, Exception):
                    self.logger.warning(f"æœç´¢ä»»åŠ¡å¤±è´¥: {results}")
                    continue
            
            # å»é‡å’Œæ’åº
            unique_results = self.search_service._deduplicate_results(all_results)
            sorted_results = self.search_service._sort_results_by_relevance(unique_results, topic)
            
            search_time = (datetime.now() - start_time).total_seconds()
            
            self.logger.info(f"æ™ºèƒ½æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(sorted_results)} ä¸ªç»“æœï¼Œè€—æ—¶: {search_time:.2f}s")
            
            return SearchResults(
                query=topic,
                results=sorted_results,
                total_count=len(sorted_results),
                search_time=search_time,
                filters_applied={
                    "intelligent_search": True,
                    "search_calls": len(search_plan),
                    "model_used": model or self.settings.default_llm_model
                }
            )
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½æœç´¢å¤±è´¥: {e}", exc_info=True)
            raise SearchException(f"æ™ºèƒ½æœç´¢å¤±è´¥: {str(e)}")
    
    async def _generate_search_plan(
        self, 
        topic: str, 
        language: str,
        model: str = None
    ) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆæœç´¢è®¡åˆ’ï¼Œè®©å¤§æ¨¡å‹å†³å®šå¦‚ä½•æœç´¢
        """
        self.logger.info(f"è®©å¤§æ¨¡å‹åˆ†æä¸»é¢˜å¹¶åˆ¶å®šæœç´¢ç­–ç•¥: {topic}")
        
        # æ˜¾ç¤ºä½¿ç”¨çš„æ¨¡å‹
        used_model = model or self.settings.default_llm_model
        self.logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {used_model}")
        
        # æ„å»ºæç¤ºè¯
        if language == "zh":
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢åŠ©æ‰‹ã€‚ç”¨æˆ·æƒ³è¦äº†è§£å…³äº"{topic}"çš„ç›¸å…³èµ„æºã€‚

**é‡è¦ï¼šä½ å¿…é¡»ä½¿ç”¨search_webå·¥å…·æ¥æœç´¢ä¿¡æ¯ï¼Œä¸è¦åªç»™æ–‡å­—å›ç­”ã€‚**

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š
1. åˆ†æ"{topic}"ä¸»é¢˜çš„ä¸åŒæ–¹é¢
2. ç«‹å³è°ƒç”¨search_webå·¥å…·è¿›è¡Œæœç´¢
3. è‡³å°‘è°ƒç”¨3-4æ¬¡æœç´¢ï¼Œè¦†ç›–ä¸åŒè§’åº¦

å¯ç”¨çš„æœç´¢ç±»å‹ï¼š
- arxiv_papers: æœç´¢arXivå­¦æœ¯è®ºæ–‡
- github_repos: æœç´¢GitHubä»£ç åº“
- huggingface_models: æœç´¢Hugging Faceæ¨¡å‹
- research_code: æœç´¢ç ”ç©¶ä»£ç 
- academic_datasets: æœç´¢å­¦æœ¯æ•°æ®é›†

ç°åœ¨è¯·ç«‹å³å¼€å§‹æœç´¢ï¼š
"""
        else:
            prompt = f"""
You are an intelligent search assistant. The user wants to learn about "{topic}".

**IMPORTANT: You MUST use the search_web tool to search for information. Do not just provide text responses.**

Please follow these steps:
1. Analyze the different aspects of "{topic}"
2. Immediately call the search_web tool to search
3. Make at least 3-4 search calls covering different angles

Available search types:
- arxiv_papers: Search arXiv academic papers
- github_repos: Search GitHub repositories
- huggingface_models: Search Hugging Face models
- research_code: Search research code
- academic_datasets: Search academic datasets

Now please start searching immediately:
"""
        
        try:
            response = await self.llm_service._call_llm(
                model=used_model,
                prompt=prompt,
                max_tokens=1000,
                temperature=0.7,
                tools=self.search_tools,
                tool_choice="auto"
            )
            
            self.logger.info(f"LLMå®Œæ•´å“åº”: {response}")
            
            # è§£æå·¥å…·è°ƒç”¨
            search_calls = self._parse_tool_calls(response)
            
            self.logger.info(f"å¤§æ¨¡å‹åˆ¶å®šäº† {len(search_calls)} ä¸ªæœç´¢è®¡åˆ’")
            
            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
            if len(search_calls) == 0:
                self.logger.warning(f"å¤§æ¨¡å‹æ²¡æœ‰è°ƒç”¨æœç´¢å·¥å…·ï¼Œå“åº”å†…å®¹: {response[:500]}")
                self.logger.info("å°†ä½¿ç”¨é™çº§æœç´¢ç­–ç•¥")
                return self._get_fallback_search_plan(topic)
            
            return search_calls
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæœç´¢è®¡åˆ’å¤±è´¥: {e}")
            # é™çº§åˆ°é»˜è®¤æœç´¢ç­–ç•¥
            return self._get_fallback_search_plan(topic)
    
    def _parse_tool_calls(self, response: str) -> List[Dict[str, Any]]:
        """
        è§£æLLMçš„å·¥å…·è°ƒç”¨å“åº”
        """
        try:
            self.logger.debug(f"è§£æå·¥å…·è°ƒç”¨å“åº”: {response}")
            
            # å°è¯•è§£æJSONæ ¼å¼çš„å·¥å…·è°ƒç”¨
            if response.strip().startswith("{"):
                data = json.loads(response)
                if "tool_calls" in data:
                    search_calls = []
                    for tool_call in data["tool_calls"]:
                        if tool_call.get("function", {}).get("name") == "search_web":
                            args_str = tool_call["function"]["arguments"]
                            # å‚æ•°å¯èƒ½å·²ç»æ˜¯å­—ç¬¦ä¸²å½¢å¼
                            if isinstance(args_str, str):
                                args = json.loads(args_str)
                            else:
                                args = args_str
                            search_calls.append(args)
                    return search_calls
            
            # å°è¯•æŸ¥æ‰¾æ–‡æœ¬ä¸­çš„å·¥å…·è°ƒç”¨æ¨¡å¼
            # æœ‰æ—¶LLMå¯èƒ½ä¼šç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°è€Œä¸æ˜¯ä¸¥æ ¼çš„JSON
            import re
            
            # å¯»æ‰¾search_webè°ƒç”¨çš„æ¨¡å¼
            patterns = [
                r'search_web\s*\(\s*query["\']?\s*:\s*["\']([^"\']+)["\']',
                r'query["\']?\s*:\s*["\']([^"\']+)["\']',
                r'æœç´¢[ï¼š:]\s*["\']?([^"\']+)["\']?'
            ]
            
            search_calls = []
            for pattern in patterns:
                matches = re.findall(pattern, response, re.IGNORECASE)
                for match in matches:
                    search_calls.append({
                        "query": match.strip(),
                        "search_type": "arxiv_papers",  # é»˜è®¤ç±»å‹
                        "max_results": 3
                    })
            
            if search_calls:
                self.logger.info(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è§£æå‡º {len(search_calls)} ä¸ªæœç´¢è°ƒç”¨")
                return search_calls[:5]  # é™åˆ¶æ•°é‡
            
            return []
            
        except Exception as e:
            self.logger.warning(f"è§£æå·¥å…·è°ƒç”¨å¤±è´¥: {e}")
            return []
    
    def _get_fallback_search_plan(self, topic: str) -> List[Dict[str, Any]]:
        """
        è·å–é»˜è®¤çš„å­¦æœ¯æœç´¢ç­–ç•¥ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        """
        self.logger.info("ä½¿ç”¨é»˜è®¤å­¦æœ¯æœç´¢ç­–ç•¥")
        
        return [
            {
                "query": f"{topic} paper",
                "search_type": "arxiv_papers",
                "max_results": 4
            },
            {
                "query": f"{topic} implementation github",
                "search_type": "github_repos", 
                "max_results": 4
            },
            {
                "query": f"{topic} research code",
                "search_type": "research_code",
                "max_results": 2
            },
            {
                "query": f"{topic} huggingface",
                "search_type": "huggingface_models",
                "max_results": 2
            }
        ]
    
    async def _execute_search_call(self, search_call: Dict[str, Any]) -> SearchResults:
        """
        æ‰§è¡Œå•ä¸ªæœç´¢è°ƒç”¨
        """
        # æ­£ç¡®è·å–æŸ¥è¯¢å‚æ•°ï¼šä»argumentsä¸­è·å–
        arguments = search_call.get("arguments", {})
        query = arguments.get("query", "")
        search_type = arguments.get("search_type", "general")
        max_results = arguments.get("max_results", 5)
        
        # éªŒè¯æŸ¥è¯¢ä¸ä¸ºç©º
        if not query or not query.strip():
            self.logger.warning(f"âš ï¸ æœç´¢æŸ¥è¯¢ä¸ºç©ºï¼Œè·³è¿‡æ‰§è¡Œ: {search_call}")
            return SearchResults(
                query="",
                results=[],
                total_count=0,
                search_time=0,
                filters_applied={"error": "Empty query"}
            )
        
        clean_query = query.strip()
        self.logger.debug(f"æ‰§è¡Œæœç´¢: {clean_query} (ç±»å‹: {search_type})")
        
        try:
            # æ ¹æ®å­¦æœ¯æœç´¢ç±»å‹è°ƒæ•´æœç´¢å‚æ•°
            include_domains = None
            exclude_domains = None
            
            if search_type == "arxiv_papers":
                include_domains = ["arxiv.org"]
                clean_query += " paper research"
            elif search_type == "github_repos":
                include_domains = ["github.com"]
                clean_query += " repository implementation"
            elif search_type == "research_code":
                include_domains = ["github.com"]
                clean_query += " code implementation paper"
            elif search_type == "academic_datasets":
                include_domains = ["github.com", "arxiv.org"]
                clean_query += " dataset data"
            elif search_type == "conference_papers":
                include_domains = ["arxiv.org"]
                clean_query += " conference paper publication"
            elif search_type == "huggingface_models":
                include_domains = ["huggingface.co"]
                clean_query += " model pre-trained"
            
            # è°ƒç”¨åŸºç¡€æœç´¢æœåŠ¡ï¼ˆå­¦æœ¯æ¨¡å¼ï¼‰
            results = await self.search_service.search_topic(
                topic=clean_query,
                max_results=max_results,
                search_depth="basic",
                include_domains=include_domains,
                exclude_domains=exclude_domains,
                academic_only=True
            )
            
            # æ·»åŠ æœç´¢ç±»å‹æ ‡è®°
            for result in results.results:
                result.source = f"{result.source}_{search_type}"
            
            return results
            
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œæœç´¢è°ƒç”¨å¤±è´¥: {e}")
            # è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return SearchResults(
                query=clean_query,
                results=[],
                total_count=0,
                search_time=0,
                filters_applied={"error": str(e)}
            )

    async def intelligent_search_with_topics(
        self,
        original_topic: str,
        extended_topic,  # ExtendedTopicå¯¹è±¡
        language: str = "zh",
        model: str = None,
        max_results: int = 20
    ) -> SearchResults:
        """
        æ ¹æ®æ‰©å±•ä¸»é¢˜è¿›è¡Œæ™ºèƒ½æœç´¢
        
        Args:
            original_topic: åŸå§‹ä¸»é¢˜
            extended_topic: æ‰©å±•ä¸»é¢˜å¯¹è±¡
            language: è¯­è¨€
            model: æŒ‡å®šçš„æ¨¡å‹
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            SearchResults: èšåˆçš„æœç´¢ç»“æœ
        """
        start_time = datetime.now()
        self.logger.info(f"ğŸ” å¼€å§‹åŸºäºæ‰©å±•ä¸»é¢˜çš„æ™ºèƒ½æœç´¢")
        
        try:
            # æ„å»ºæœç´¢è®¡åˆ’ï¼šä½¿ç”¨æ‰©å±•çš„å…³é”®è¯å’Œç›¸å…³æ¦‚å¿µ
            search_plan = self._generate_search_plan_from_topics(
                original_topic, 
                extended_topic, 
                language, 
                model
            )
            
            # æ‰§è¡Œæœç´¢è®¡åˆ’
            all_results = []
            search_tasks = []
            
            for search_call in search_plan:
                task = self._execute_search_call(search_call)
                search_tasks.append(task)
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æœç´¢ä»»åŠ¡
            search_results_list = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            # åˆå¹¶æœç´¢ç»“æœ
            for results in search_results_list:
                if isinstance(results, SearchResults):
                    all_results.extend(results.results)
                elif isinstance(results, Exception):
                    self.logger.warning(f"âš ï¸ æœç´¢ä»»åŠ¡å¤±è´¥: {results}")
                    continue
            
            # å»é‡å’Œæ’åº
            unique_results = self.search_service._deduplicate_results(all_results)
            sorted_results = self.search_service._sort_results_by_relevance(unique_results, original_topic)
            
            # é™åˆ¶ç»“æœæ•°é‡
            final_results = sorted_results[:max_results]
            
            search_time = (datetime.now() - start_time).total_seconds()
            
            self.logger.info(f"ğŸ‰ åŸºäºæ‰©å±•ä¸»é¢˜çš„æ™ºèƒ½æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_results)} ä¸ªç»“æœï¼Œè€—æ—¶: {search_time:.2f}s")
            
            return SearchResults(
                query=original_topic,
                results=final_results,
                total_count=len(final_results),
                search_time=search_time,
                filters_applied={
                    "intelligent_search_with_topics": True,
                    "search_calls": len(search_plan),
                    "model_used": model or self.settings.default_llm_model,
                    "extended_keywords": len(extended_topic.extended_keywords),
                    "related_concepts": len(extended_topic.related_concepts)
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ åŸºäºæ‰©å±•ä¸»é¢˜çš„æ™ºèƒ½æœç´¢å¤±è´¥: {e}", exc_info=True)
            raise SearchException(f"æ™ºèƒ½æœç´¢å¤±è´¥: {str(e)}")

    def _generate_search_plan_from_topics(
        self, 
        original_topic: str,
        extended_topic,  # ExtendedTopicå¯¹è±¡
        language: str,
        model: str = None
    ) -> List[Dict[str, Any]]:
        """
        åŸºäºæ‰©å±•ä¸»é¢˜ç”Ÿæˆæœç´¢è®¡åˆ’
        """
        self.logger.info(f"ğŸ“ åŸºäºæ‰©å±•ä¸»é¢˜åˆ¶å®šæœç´¢è®¡åˆ’")
        self.logger.info(f"ğŸ”‘ æ‰©å±•å…³é”®è¯: {extended_topic.extended_keywords}")
        self.logger.info(f"ğŸ’¡ ç›¸å…³æ¦‚å¿µ: {extended_topic.related_concepts}")
        
        search_calls = []
        
        # ä¸ºæ¯ä¸ªæ‰©å±•å…³é”®è¯å’Œç›¸å…³æ¦‚å¿µåˆ›å»ºæœç´¢è°ƒç”¨
        all_search_terms = []
        
        # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²å’ŒNoneå€¼
        valid_keywords = [kw.strip() for kw in extended_topic.extended_keywords if kw and kw.strip()]
        valid_concepts = [concept.strip() for concept in extended_topic.related_concepts if concept and concept.strip()]
        
        self.logger.info(f"âœ… æœ‰æ•ˆå…³é”®è¯: {valid_keywords}")
        self.logger.info(f"âœ… æœ‰æ•ˆæ¦‚å¿µ: {valid_concepts}")
        
        all_search_terms.extend(valid_keywords[:3])  # æœ€å¤š3ä¸ªå…³é”®è¯
        all_search_terms.extend(valid_concepts[:2])   # æœ€å¤š2ä¸ªæ¦‚å¿µ
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æœç´¢è¯ï¼Œä½¿ç”¨åŸå§‹ä¸»é¢˜
        if not all_search_terms:
            self.logger.warning(f"âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ‰©å±•æœç´¢è¯ï¼Œä½¿ç”¨åŸå§‹ä¸»é¢˜: {original_topic}")
            all_search_terms = [original_topic]
        
        # ä¸ºæ¯ä¸ªæœç´¢è¯åˆ›å»ºä¸åŒç±»å‹çš„æœç´¢
        search_types = ["arxiv_papers", "github_repos", "huggingface_models", "academic_datasets"]
        
        for i, search_term in enumerate(all_search_terms):
            # ç¡®ä¿æœç´¢è¯ä¸ä¸ºç©º
            if not search_term or not search_term.strip():
                self.logger.warning(f"âš ï¸ è·³è¿‡ç©ºæœç´¢è¯: '{search_term}'")
                continue
                
            clean_search_term = search_term.strip()
            self.logger.debug(f"ğŸ” ä¸ºæœç´¢è¯ '{clean_search_term}' åˆ›å»ºæœç´¢è®¡åˆ’")
            
            # æ¯ä¸ªæœç´¢è¯ä½¿ç”¨2ç§æœç´¢ç±»å‹
            for j, search_type in enumerate(search_types[:2]):
                search_calls.append({
                    "function": "search_web",
                    "arguments": {
                        "query": clean_search_term,
                        "search_type": search_type,
                        "max_results": 3
                    }
                })
                
                # é™åˆ¶æ€»æœç´¢æ¬¡æ•°
                if len(search_calls) >= 8:
                    break
            
            if len(search_calls) >= 8:
                break
        
        self.logger.info(f"ğŸ“Š åˆ¶å®šäº† {len(search_calls)} ä¸ªæœç´¢è®¡åˆ’")
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæœç´¢è®¡åˆ’
        if not search_calls:
            self.logger.warning(f"âš ï¸ æ²¡æœ‰ç”Ÿæˆæœç´¢è®¡åˆ’ï¼Œä½¿ç”¨åŸå§‹ä¸»é¢˜: {original_topic}")
            search_calls = [{
                "function": "search_web",
                "arguments": {
                    "query": original_topic,
                    "search_type": "arxiv_papers",
                    "max_results": 5
                }
            }]
        
        return search_calls 