"""
å¤§è¯­è¨€æ¨¡å‹æœåŠ¡æ¨¡å—
é›†æˆOpenAIå’ŒDeepSeek APIå®ç°æ™ºèƒ½å†…å®¹ç”Ÿæˆ
"""

import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime

import openai
import httpx

from app.models import ExtendedTopic, SearchResults
from app.utils import get_settings, get_logger, LLMException, APIException, LoggerMixin


class LLMService(LoggerMixin):
    """
    å¤§è¯­è¨€æ¨¡å‹æœåŠ¡ç±»
    è´Ÿè´£é›†æˆå¤šä¸ªLLMæä¾›å•†è¿›è¡Œæ™ºèƒ½å†…å®¹ç”Ÿæˆ
    """

    def __init__(self):
        self.settings = get_settings()

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.openai_client = openai.AsyncOpenAI(
            api_key=self.settings.openai_api_key,
            timeout=self.settings.request_timeout
        )

        # DeepSeekä½¿ç”¨OpenAIå…¼å®¹çš„API
        self.deepseek_client = openai.AsyncOpenAI(
            api_key=self.settings.deepseek_api_key,
            base_url="https://api.deepseek.com/v1",
            timeout=self.settings.request_timeout
        )

    async def expand_topic(self, topic: str, language: str = "zh") -> ExtendedTopic:
        """
        æ‰©å±•ä¸»é¢˜ï¼Œç”Ÿæˆç›¸å…³å…³é”®è¯å’Œæœç´¢æŸ¥è¯¢

        Args:
            topic: åŸå§‹ä¸»é¢˜
            language: è¯­è¨€ (zh/en)

        Returns:
            ExtendedTopic: æ‰©å±•åçš„ä¸»é¢˜ä¿¡æ¯
        """
        self.logger.info(f"ğŸ” å¼€å§‹æ‰©å±•ä¸»é¢˜: {topic}")

        try:
            prompt = self._build_topic_expansion_prompt(topic, language)

            # ä½¿ç”¨é»˜è®¤æ¨¡å‹è¿›è¡Œä¸»é¢˜æ‰©å±•
            response = await self._call_llm(
                model=self.settings.default_llm_model,
                prompt=prompt,
                max_tokens=500,
                temperature=0.7
            )

            # è®°å½•LLMåŸå§‹å“åº”
            self.logger.info(f"ğŸ“ ä¸»é¢˜æ‰©å±•åŸå§‹å“åº”:\n{response}")

            # è§£æå“åº”
            expanded_topic = self._parse_topic_expansion_response(
                response, topic)

            # è¯¦ç»†è®°å½•æ‰©å±•ç»“æœ
            self.logger.info(f"âœ… ä¸»é¢˜æ‰©å±•å®Œæˆ!")
            self.logger.info(f"ğŸ”‘ æ‰©å±•å…³é”®è¯ ({len(expanded_topic.extended_keywords)}ä¸ª): {expanded_topic.extended_keywords}")
            self.logger.info(f"ğŸ’¡ ç›¸å…³æ¦‚å¿µ ({len(expanded_topic.related_concepts)}ä¸ª): {expanded_topic.related_concepts}")
            self.logger.info(f"ğŸ” æœç´¢æŸ¥è¯¢ ({len(expanded_topic.search_queries)}ä¸ª): {expanded_topic.search_queries}")
            
            return expanded_topic

        except Exception as e:
            self.logger.error(f"âŒ ä¸»é¢˜æ‰©å±•å¤±è´¥: {e}", exc_info=True)
            # è¿”å›é»˜è®¤çš„æ‰©å±•ç»“æœï¼Œä½†è®°å½•è¯¦ç»†ä¿¡æ¯
            fallback_topic = ExtendedTopic(
                original_topic=topic,
                extended_keywords=[topic],
                related_concepts=[],
                search_queries=[topic, f"{topic} tutorial", f"awesome {topic}"]
            )
            self.logger.warning(f"ğŸ”„ ä½¿ç”¨é»˜è®¤æ‰©å±•ç»“æœ: {fallback_topic.extended_keywords}")
            return fallback_topic

    async def generate_awesome_list(
        self,
        topic: str,
        search_results: SearchResults,
        language: str = "zh",
        model: str = None
    ) -> str:
        """
        æ ¹æ®æœç´¢ç»“æœç”ŸæˆAwesome List

        Args:
            topic: ä¸»é¢˜
            search_results: æœç´¢ç»“æœ
            language: è¯­è¨€
            model: æŒ‡å®šçš„æ¨¡å‹

        Returns:
            str: ç”Ÿæˆçš„Markdownæ ¼å¼Awesome List
        """
        self.logger.info(f"å¼€å§‹ç”ŸæˆAwesome List: {topic}")

        try:
            prompt = self._build_awesome_list_prompt(
                topic, search_results, language)

            used_model = model or self.settings.default_llm_model

            response = await self._call_llm(
                model=used_model,
                prompt=prompt,
                max_tokens=2000,
                temperature=0.3  # è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º
            )

            # åå¤„ç†ç”Ÿæˆçš„å†…å®¹
            awesome_list = self._post_process_awesome_list(response, topic)

            self.logger.info(f"Awesome Listç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(awesome_list)} å­—ç¬¦")
            return awesome_list

        except Exception as e:
            self.logger.error(f"Awesome Listç”Ÿæˆå¤±è´¥: {e}")
            raise LLMException(f"ç”ŸæˆAwesome Listå¤±è´¥: {str(e)}")

    async def extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯

        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡

        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        try:
            prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æœ€é‡è¦çš„ {max_keywords} ä¸ªå…³é”®è¯ï¼Œè¿”å›JSONæ ¼å¼ï¼š

æ–‡æœ¬ï¼š
{text[:1000]}  # é™åˆ¶æ–‡æœ¬é•¿åº¦

è¯·è¿”å›æ ¼å¼ï¼š
{{"keywords": ["å…³é”®è¯1", "å…³é”®è¯2", ...]}}
"""

            response = await self._call_llm(
                model=self.settings.default_llm_model,
                prompt=prompt,
                max_tokens=200,
                temperature=0.1
            )

            # ç®€å•è§£æå…³é”®è¯
            import json
            try:
                parsed = json.loads(response)
                return parsed.get("keywords", [])[:max_keywords]
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ–‡æœ¬åˆ†å‰²
                keywords = [word.strip().strip('"\'')
                            for word in response.split('\n') if word.strip()]
                return keywords[:max_keywords]

        except Exception as e:
            self.logger.warning(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            return []

    async def _call_llm(
        self,
        model: str,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: str = "auto"
    ) -> str:
        """
        è°ƒç”¨æŒ‡å®šçš„å¤§è¯­è¨€æ¨¡å‹

        Args:
            model: æ¨¡å‹åç§° (gpt/deepseek)
            prompt: æç¤ºè¯
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆFunction Callingï¼‰
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥

        Returns:
            str: æ¨¡å‹å“åº”
        """
        try:
            # è®°å½•æ¨¡å‹è°ƒç”¨ä¿¡æ¯
            tools_info = f"ï¼Œå·¥å…·æ•°é‡: {len(tools) if tools else 0}" if tools else ""
            self.logger.info(
                f"ğŸ”§ è°ƒç”¨LLM: {model.upper()}{tools_info}ï¼Œæ¸©åº¦: {temperature}")
            # æ„å»ºè¯·æ±‚å‚æ•°
            messages = [{"role": "user", "content": prompt}]
            request_params = {
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature
            }

            # æ·»åŠ å·¥å…·å®šä¹‰ï¼ˆå¦‚æœæä¾›ï¼‰
            if tools:
                request_params["tools"] = tools
                request_params["tool_choice"] = tool_choice

            if model.lower() == "gpt":
                # ä½¿ç”¨GPT-4 Turboï¼ŒFunction Callingèƒ½åŠ›æ›´å¼º
                actual_model = "gpt-4-turbo-preview"
                self.logger.info(
                    f"ğŸ“¡ å®é™…è°ƒç”¨æ¨¡å‹: {actual_model} (æ”¯æŒå¼ºåŒ–Function Calling)")
                response = await self.openai_client.chat.completions.create(
                    model=actual_model,
                    **request_params
                )
                return self._process_llm_response(response)

            elif model.lower() == "deepseek":
                actual_model = "deepseek-chat"
                self.logger.debug(f"ğŸ“¡ å®é™…è°ƒç”¨æ¨¡å‹: {actual_model}")
                response = await self.deepseek_client.chat.completions.create(
                    model=actual_model,
                    **request_params
                )
                return self._process_llm_response(response)

            else:
                raise LLMException(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}")

        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥ ({model}): {e}")
            raise APIException(f"LLM APIè°ƒç”¨å¤±è´¥: {str(e)}")

    def _process_llm_response(self, response) -> str:
        """
        å¤„ç†LLMå“åº”ï¼Œæ”¯æŒFunction Calling
        """
        choice = response.choices[0]
        message = choice.message

        # å¦‚æœæ˜¯æ™®é€šæ–‡æœ¬å“åº”
        if message.content:
            return message.content

        # å¦‚æœæ˜¯å·¥å…·è°ƒç”¨å“åº”
        if hasattr(message, 'tool_calls') and message.tool_calls:
            # è¿”å›å·¥å…·è°ƒç”¨ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
            import json
            tool_calls = []
            for tool_call in message.tool_calls:
                tool_calls.append({
                    "id": tool_call.id,
                    "function": {
                        "name": tool_call.function.name,
                        "arguments": tool_call.function.arguments
                    }
                })
            return json.dumps({"tool_calls": tool_calls})

        return ""

    def _build_topic_expansion_prompt(self, topic: str, language: str) -> str:
        """
        æ„å»ºä¸»é¢˜æ‰©å±•çš„æç¤ºè¯
        """
        if language == "zh":
            return f"""
ä½ æ˜¯ä¸€ä¸ªä¸»é¢˜ç ”ç©¶ä¸“å®¶ã€‚è¯·å¸®æˆ‘æ‰©å±•ä»¥ä¸‹ä¸»é¢˜ï¼Œç”Ÿæˆç›¸å…³çš„å…³é”®è¯ã€æ¦‚å¿µå’Œæœç´¢æŸ¥è¯¢ã€‚

ä¸»é¢˜ï¼š{topic}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š

æ‰©å±•å…³é”®è¯ï¼š
- å…³é”®è¯1
- å…³é”®è¯2
- å…³é”®è¯3

ç›¸å…³æ¦‚å¿µï¼š
- æ¦‚å¿µ1
- æ¦‚å¿µ2
- æ¦‚å¿µ3

æ¨èæœç´¢æŸ¥è¯¢ï¼š
- æŸ¥è¯¢1
- æŸ¥è¯¢2
- æŸ¥è¯¢3

è¦æ±‚ï¼š
1. å…³é”®è¯åº”è¯¥æ¶µç›–ä¸»é¢˜çš„ä¸åŒæ–¹é¢
2. ç›¸å…³æ¦‚å¿µåº”è¯¥æ˜¯ä¸ä¸»é¢˜ç›¸å…³çš„æŠ€æœ¯æœ¯è¯­æˆ–æ¦‚å¿µ
3. æœç´¢æŸ¥è¯¢åº”è¯¥èƒ½å¤Ÿæ‰¾åˆ°é«˜è´¨é‡çš„èµ„æº
4. æ¯ç±»æä¾›3-5ä¸ªé¡¹ç›®å³å¯
"""
        else:
            return f"""
You are a topic research expert. Please help me expand the following topic by generating related keywords, concepts, and search queries.

Topic: {topic}

Please return in the following format:

Extended Keywords:
- keyword1
- keyword2
- keyword3

Related Concepts:
- concept1
- concept2
- concept3

Recommended Search Queries:
- query1
- query2
- query3

Requirements:
1. Keywords should cover different aspects of the topic
2. Related concepts should be technical terms or concepts related to the topic
3. Search queries should be able to find high-quality resources
4. Provide 3-5 items for each category
"""

    def _build_awesome_list_prompt(
        self,
        topic: str,
        search_results: SearchResults,
        language: str
    ) -> str:
        """
        æ„å»ºç”ŸæˆAwesome Listçš„æç¤ºè¯
        """
        # å‡†å¤‡æœç´¢ç»“æœæ‘˜è¦
        results_summary = []
        for i, result in enumerate(search_results.results[:10], 1):  # é™åˆ¶ç»“æœæ•°é‡
            results_summary.append(
                f"{i}. [{result.title}]({result.url}) - {result.source} - {result.content[:200]}..."
            )

        results_text = "\n".join(results_summary)

        if language == "zh":
            return f"""
ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯èµ„æºæ•´ç†ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æœç´¢ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„ Awesome {topic} åˆ—è¡¨ã€‚

æœç´¢ç»“æœï¼š
{results_text}

è¯·ç”Ÿæˆä¸€ä¸ªMarkdownæ ¼å¼çš„Awesome Listï¼Œè¦æ±‚ï¼š

1. ä½¿ç”¨æ ‡å‡†çš„Awesome Listæ ¼å¼
2. åŒ…å«æ¸…æ™°çš„åˆ†ç±»ï¼ˆå¦‚ï¼šå®˜æ–¹èµ„æºã€å·¥å…·åº“ã€æ•™ç¨‹æ–‡æ¡£ã€é¡¹ç›®ç¤ºä¾‹ç­‰ï¼‰
3. æ¯ä¸ªé“¾æ¥éƒ½è¦æœ‰ç®€æ´çš„æè¿°
4. ä¼˜å…ˆé€‰æ‹©æœ€æƒå¨å’Œæœ‰ç”¨çš„èµ„æº
5. ç¡®ä¿é“¾æ¥çš„å‡†ç¡®æ€§
6. æ·»åŠ é€‚å½“çš„emojiå›¾æ ‡
7. åŒ…å«ç®€ä»‹éƒ¨åˆ†
8. ä¿æŒä¸“ä¸šå’Œå‡†ç¡®

æ ¼å¼ç¤ºä¾‹ï¼š
# Awesome {topic}

> ç²¾é€‰çš„ {topic} ç›¸å…³èµ„æºåˆ—è¡¨

## ğŸ“š å®˜æ–¹èµ„æº
- [èµ„æºåç§°](é“¾æ¥) - ç®€æ´æè¿°

## ğŸ› ï¸ å·¥å…·å’Œåº“
- [å·¥å…·åç§°](é“¾æ¥) - ç®€æ´æè¿°

è¯·ç¡®ä¿ç”Ÿæˆçš„å†…å®¹å‡†ç¡®ã€æœ‰ç”¨ä¸”ç»“æ„æ¸…æ™°ã€‚
"""
        else:
            return f"""
You are a technical resource curator. Please generate a high-quality Awesome {topic} list based on the following search results.

Search Results:
{results_text}

Please generate a Markdown-formatted Awesome List with the following requirements:

1. Use standard Awesome List format
2. Include clear categories (e.g., Official Resources, Tools & Libraries, Documentation, Examples)
3. Each link should have a concise description
4. Prioritize the most authoritative and useful resources
5. Ensure link accuracy
6. Add appropriate emoji icons
7. Include an introduction section
8. Maintain professionalism and accuracy

Format example:
# Awesome {topic}

> A curated list of {topic} resources

## ğŸ“š Official Resources
- [Resource Name](link) - Brief description

## ğŸ› ï¸ Tools & Libraries
- [Tool Name](link) - Brief description

Please ensure the generated content is accurate, useful, and well-structured.
"""

    def _parse_topic_expansion_response(self, response: str, original_topic: str) -> ExtendedTopic:
        """
        è§£æä¸»é¢˜æ‰©å±•å“åº”
        """
        try:
            self.logger.info(f"ğŸ” å¼€å§‹è§£æä¸»é¢˜æ‰©å±•å“åº”...")
            lines = response.strip().split('\n')

            extended_keywords = []
            related_concepts = []
            search_queries = []

            current_section = None

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                if "æ‰©å±•å…³é”®è¯" in line or "Extended Keywords" in line:
                    current_section = "keywords"
                    self.logger.debug(f"ğŸ“ è¿›å…¥æ‰©å±•å…³é”®è¯éƒ¨åˆ†")
                elif "ç›¸å…³æ¦‚å¿µ" in line or "Related Concepts" in line:
                    current_section = "concepts"
                    self.logger.debug(f"ğŸ“ è¿›å…¥ç›¸å…³æ¦‚å¿µéƒ¨åˆ†")
                elif "æœç´¢æŸ¥è¯¢" in line or "Search Queries" in line or "æ¨èæœç´¢æŸ¥è¯¢" in line or "Recommended Search Queries" in line:
                    current_section = "queries"
                    self.logger.debug(f"ğŸ“ è¿›å…¥æœç´¢æŸ¥è¯¢éƒ¨åˆ†")
                elif line.startswith('- '):
                    item = line[2:].strip()
                    # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²
                    if not item:
                        self.logger.debug(f"âš ï¸ è·³è¿‡ç©ºé¡¹ç›®")
                        continue
                        
                    if current_section == "keywords":
                        extended_keywords.append(item)
                        self.logger.debug(f"ğŸ”‘ æ·»åŠ å…³é”®è¯: {item}")
                    elif current_section == "concepts":
                        related_concepts.append(item)
                        self.logger.debug(f"ğŸ’¡ æ·»åŠ æ¦‚å¿µ: {item}")
                    elif current_section == "queries":
                        search_queries.append(item)
                        self.logger.debug(f"ğŸ” æ·»åŠ æŸ¥è¯¢: {item}")

            # è¿‡æ»¤æ‰æ‰€æœ‰ç©ºå­—ç¬¦ä¸²
            extended_keywords = [kw for kw in extended_keywords if kw and kw.strip()]
            related_concepts = [concept for concept in related_concepts if concept and concept.strip()]
            search_queries = [query for query in search_queries if query and query.strip()]

            # è®°å½•è§£æç»Ÿè®¡
            self.logger.info(f"ğŸ“Š è§£æç»Ÿè®¡ - å…³é”®è¯:{len(extended_keywords)}, æ¦‚å¿µ:{len(related_concepts)}, æŸ¥è¯¢:{len(search_queries)}")

            # ç¡®ä¿åŒ…å«åŸå§‹ä¸»é¢˜
            if original_topic and original_topic not in extended_keywords:
                extended_keywords.insert(0, original_topic)
                self.logger.debug(f"ğŸ”„ æ·»åŠ åŸå§‹ä¸»é¢˜åˆ°å…³é”®è¯: {original_topic}")

            # å¦‚æœè§£æå¤±è´¥ï¼Œæä¾›é»˜è®¤å€¼
            if not extended_keywords:
                extended_keywords = [original_topic] if original_topic else ["general topic"]
                self.logger.warning(f"âš ï¸ æœªè§£æåˆ°å…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤å€¼: {extended_keywords}")
            if not search_queries:
                search_queries = [original_topic, f"{original_topic} tutorial"] if original_topic else ["general search"]
                self.logger.warning(f"âš ï¸ æœªè§£æåˆ°æŸ¥è¯¢ï¼Œä½¿ç”¨é»˜è®¤å€¼: {search_queries}")

            result = ExtendedTopic(
                original_topic=original_topic,
                extended_keywords=extended_keywords,
                related_concepts=related_concepts,
                search_queries=search_queries
            )
            
            self.logger.info(f"âœ… ä¸»é¢˜æ‰©å±•å“åº”è§£æå®Œæˆ")
            self.logger.info(f"ğŸ¯ æœ€ç»ˆå…³é”®è¯: {result.extended_keywords}")
            self.logger.info(f"ğŸ¯ æœ€ç»ˆæ¦‚å¿µ: {result.related_concepts}")
            
            return result

        except Exception as e:
            self.logger.error(f"âŒ è§£æä¸»é¢˜æ‰©å±•å“åº”å¤±è´¥: {e}", exc_info=True)
            fallback_result = ExtendedTopic(
                original_topic=original_topic,
                extended_keywords=[original_topic] if original_topic else ["fallback topic"],
                related_concepts=[],
                search_queries=[original_topic, f"{original_topic} tutorial", f"awesome {original_topic}"] if original_topic else ["fallback search"]
            )
            self.logger.warning(f"ğŸ”„ ä½¿ç”¨è§£æå¤±è´¥æ—¶çš„é»˜è®¤ç»“æœ: {fallback_result.extended_keywords}")
            return fallback_result

    def _post_process_awesome_list(self, content: str, topic: str) -> str:
        """
        åå¤„ç†ç”Ÿæˆçš„Awesome List
        """
        # ç¡®ä¿æœ‰æ ‡é¢˜
        if not content.startswith("#"):
            content = f"# Awesome {topic}\n\n{content}"

        # ç¡®ä¿æœ‰ç”Ÿæˆæ ‡è¯†
        if "Awesome List Agent" not in content:
            content += "\n\n---\n*ç”± Awesome List Agent æ™ºèƒ½ç”Ÿæˆ*"

        return content.strip()
